{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protect your phone from spam messages with Naive Bayes\n",
    "## Introduction and motivation\n",
    "In this project, we are going to build a spam filter using Naive Bayes classifiers along with a dataset of 5,572 SMS messages that are already classified by humans.\n",
    "The dataset is collected by Tiago A. Almeida and José María Gómez Hidalgo from various source and can be downloaded from here:\n",
    "https://archive.ics.uci.edu/ml/datasets/sms+spam+collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset overview\n",
    "The dataset contains 2 columns and 5,572 rows:\n",
    "- Label for sms\n",
    "- Sms content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import dataset and necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "sms = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['Label', 'SMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.593683\n",
       "spam    13.406317\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What percentage of the messages is spam and what percentage is ham(non-spam)\n",
    "sms.Label.value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have looked in the dataset and saw that about 87% of the messages are ham (non-spam), and the remaining 13% are spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train set and test set\n",
    "In order to avoid biased result, we need to divide the dataset into a train set (80%) and test set (20%). We will leave the test set until the very end to prevent our algorithm from trying to tweak and fit into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Randomizing and split the dataset\n",
    "randomized_data = sms.sample(frac=1, random_state=1)\n",
    "train_index = round(len(randomized_data)*0.8)\n",
    "train_set = randomized_data[:train_index].reset_index(drop=True)\n",
    "test_set = randomized_data[train_index:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.54105\n",
       "spam    13.45895\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the spam and non-spam percentage in train set\n",
    "train_set.Label.value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.804309\n",
       "spam    13.195691\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the spam and non-spam percentage in test set\n",
    "test_set.Label.value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We conclude that both training set and test set have similar percentages compared to the full dataset. Now let's move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "In order to make Naive Bayes classifiers for each word in a new message, we want to:\n",
    "- Normalize all the letters to lowercase\n",
    "- Remove special characters such as !*#@^&\n",
    "- Bring data to this format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://dq-content.s3.amazonaws.com/433/cpgp_dataset_2.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row describes a single message instead of 'SMS' column. For instance, the first row corresponds to the message \"SECRET PRIZE! CLAIM SECRET PRIZE NOW!!\", and it has the values spam, 2, 2, 1, 1, 0, 0, 0, 0, 0. These values tell us that:\n",
    "- The message is spam.\n",
    "- The word \"secret\" occurs two times inside the message.\n",
    "- The word \"prize\" occurs two times inside the message.\n",
    "- etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize the text and remove special characters\n",
    "import re\n",
    "train_set['SMS'] = train_set['SMS'].str.replace('\\W', ' ')\n",
    "train_set['SMS'] = train_set['SMS'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the vocabulary\n",
    "Make a list of unique words from the whole training set in order to bring our dataset to the desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set['SMS'] = train_set['SMS'].str.split()\n",
    "vocabulary = []\n",
    "for r in train_set['SMS']:\n",
    "    for w in r:\n",
    "        vocabulary.append(w)\n",
    "vocabulary = list(set(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary word_counts_per_txt where each key is a unique word from the vocabulary, and each value is a list of the length of training set, where each element in the list is the count of that word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>02</th>\n",
       "      <th>0207</th>\n",
       "      <th>02072069400</th>\n",
       "      <th>...</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>é</th>\n",
       "      <th>ú1</th>\n",
       "      <th>ü</th>\n",
       "      <th>〨ud</th>\n",
       "      <th>鈥</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 7783 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  00  000  000pes  008704050406  0089  01223585334  02  0207  02072069400  \\\n",
       "0  0   0    0       0             0     0            0   0     0            0   \n",
       "1  0   0    0       0             0     0            0   0     0            0   \n",
       "2  0   0    0       0             0     0            0   0     0            0   \n",
       "\n",
       "  ...  zindgi  zoe  zogtorius  zouk  zyada  é  ú1  ü  〨ud  鈥  \n",
       "0 ...       0    0          0     0      0  0   0  0    0  0  \n",
       "1 ...       0    0          0     0      0  0   0  0    0  0  \n",
       "2 ...       0    0          0     0      0  0   0  0    0  0  \n",
       "\n",
       "[3 rows x 7783 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts_per_txt = {unique_word: [0] * len(train_set['SMS']) for unique_word in vocabulary}\n",
    "for index, text in enumerate(train_set['SMS']):\n",
    "    for word in text:\n",
    "        word_counts_per_txt[word][index] += 1\n",
    "# Transform word_counts_per_txt into a Dataframe\n",
    "word_counts = pd.DataFrame(word_counts_per_txt)\n",
    "word_counts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the label is missing? Let's concatenate word_counts dataframe with training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = pd.concat([train_set, word_counts], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Implementing Naive Bayes classification\n",
    "The formula below is the Naive Bayes for spam and ham probability for any new messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://render.githubusercontent.com/render/math?math=P%28Spam%20%7C%20w_1%2Cw_2%2C%20...%2C%20w_n%29%20%5Cpropto%20P%28Spam%29%20%5Ccdot%20%5Cprod_%7Bi%3D1%7D%5E%7Bn%7DP%28w_i%7CSpam%29&mode=display\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://render.githubusercontent.com/render/math?math=P%28Ham%20%7C%20w_1%2Cw_2%2C%20...%2C%20w_n%29%20%5Cpropto%20P%28Ham%29%20%5Ccdot%20%5Cprod_%7Bi%3D1%7D%5E%7Bn%7DP%28w_i%7CHam%29&mode=display\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://render.githubusercontent.com/render/math?math=P%28w_i%7CSpam%29%20%3D%20%5Cfrac%7BN_%7Bw_i%7CSpam%7D%20%2B%20%5Calpha%7D%7BN_%7BSpam%7D%20%2B%20%5Calpha%20%5Ccdot%20N_%7BVocabulary%7D%7D&mode=display\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://render.githubusercontent.com/render/math?math=P%28w_i%7CHam%29%20%3D%20%5Cfrac%7BN_%7Bw_i%7CHam%7D%20%2B%20%5Calpha%7D%7BN_%7BHam%7D%20%2B%20%5Calpha%20%5Ccdot%20N_%7BVocabulary%7D%7D&mode=display\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's caculate all the necessary variables such as:\n",
    "- p_ham: probability of ham sms\n",
    "- p_spam: probability of spam sms\n",
    "- n_ham: the number of words in all the ham messages, not to be confused with the number of unique words in ham messages\n",
    "- n_spam: the number of words in all the spam messages, not to be confused with the number of unique words in spam messages\n",
    "- alpha: Laplace smoothing\n",
    "- p_word_ham: a dictionary for the probability of each unique word that appear in a ham message\n",
    "- p_word_ham: a dictionary for the probability of each unique word that appear in a spam message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Caculate p_ham, p_spam, n_ham and n_spam\n",
    "ham_sms = train_set[train_set.Label == 'ham']\n",
    "spam_sms = train_set[train_set.Label == 'spam']\n",
    "p_ham = len(spam_sms) / len(train_set)\n",
    "p_spam = len(spam_sms) / len(train_set)\n",
    "n_ham = ham_sms['SMS'].apply(len).sum()\n",
    "n_spam = spam_sms['SMS'].apply(len).sum()\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize two dictionaries p_word_ham and p_word_spam\n",
    "p_word_ham = {unique_word: 0 for unique_word in vocabulary}\n",
    "p_word_spam = {unique_word: 0 for unique_word in vocabulary}\n",
    "for word in vocabulary:\n",
    "    n_word_given_ham = ham_sms[word].sum()\n",
    "    p_word_given_ham = (n_word_given_ham + alpha) /(n_ham + alpha * len(vocabulary))\n",
    "    p_word_ham[word] = p_word_given_ham\n",
    "    n_word_given_spam = spam_sms[word].sum()\n",
    "    p_word_given_spam = (n_word_given_spam + alpha) /(n_spam + alpha * len(vocabulary))\n",
    "    p_word_spam[word] = p_word_given_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the constants and variable we needed.Let's create our spam filter. We can start buiding a function that:\n",
    "- Take in a new meassage as input\n",
    "- Caculate P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn)\n",
    "- Compares the values of P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn) to decide on label of new message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification(mes):\n",
    "    sms = re.sub('\\W', ' ', mes)\n",
    "    sms = sms.lower()\n",
    "    sms = sms.split()\n",
    "    p_ham_given_message = p_ham\n",
    "    p_spam_given_message = p_spam\n",
    "    for w in sms:\n",
    "        if w in p_word_ham:\n",
    "            p_ham_given_message *= p_word_ham[w]\n",
    "        if w in p_word_spam:\n",
    "            p_spam_given_message *= p_word_spam[w]\n",
    "    print('P Ham: ', p_ham_given_message)\n",
    "    print('P Spam: ', p_spam_given_message)\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        label = 'Ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        label = 'Spam'\n",
    "    else:\n",
    "        label = 'This needs human evaluation'\n",
    "    print('Label: ', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P Ham:  3.4799505962427803e-29\n",
      "P Spam:  2.258443385393428e-24\n",
      "Label:  Spam\n"
     ]
    }
   ],
   "source": [
    "# Test on message 1\n",
    "classification('WINNER!! This is the secret code to unlock the money: C3421.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P Ham:  8.054242025684071e-22\n",
      "P Spam:  1.2514868586939768e-25\n",
      "Label:  Ham\n"
     ]
    }
   ],
   "source": [
    "# Test on message 2\n",
    "classification('Sounds good, Tom, then see u there')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the function onto our test set and get our accuracy for this classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.29443447037703"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classification_test_set(mes):\n",
    "    sms = re.sub('\\W', ' ', mes)\n",
    "    sms = sms.lower()\n",
    "    sms = sms.split()\n",
    "    p_ham_given_message = p_ham\n",
    "    p_spam_given_message = p_spam\n",
    "    for w in sms:\n",
    "        if w in p_word_ham:\n",
    "            p_ham_given_message *= p_word_ham[w]\n",
    "        if w in p_word_spam:\n",
    "            p_spam_given_message *= p_word_spam[w]\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'This needs human evaluation'\n",
    "test_set['prediction'] = test_set['SMS'].apply(classification_test_set)\n",
    "correct =0\n",
    "total = len(test_set)\n",
    "for i in test_set.iterrows():\n",
    "    i = i[1]\n",
    "    if i['Label'] == i['prediction']:\n",
    "        correct += 1\n",
    "correct/total * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and next step\n",
    "So the accuracy is at 97,6% percent. That is quite impressive for such a small training set. Let's have a look at the inaccurately marked data to see what we can improve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>spam</td>\n",
       "      <td>Not heard from U4 a while. Call me now am here...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>spam</td>\n",
       "      <td>More people are dogging in your area now. Call...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>ham</td>\n",
       "      <td>Unlimited texts. Limited minutes.</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>ham</td>\n",
       "      <td>26th OF JULY</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>ham</td>\n",
       "      <td>Surely result will offer:)</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>ham</td>\n",
       "      <td>Which channel:-):-):):-).</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nokia phone is lovly..</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>ham</td>\n",
       "      <td>A Boy loved a gal. He propsd bt she didnt mind...</td>\n",
       "      <td>This needs human evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>ham</td>\n",
       "      <td>No calls..messages..missed calls</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>ham</td>\n",
       "      <td>This phone has the weirdest auto correct.</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>ham</td>\n",
       "      <td>We have sent JD for Customer Service cum Accou...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>ham</td>\n",
       "      <td>Just taste fish curry :-P</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>ham</td>\n",
       "      <td>No. Yes please. Been swimming?</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>spam</td>\n",
       "      <td>You won't believe it but it's true. It's Incre...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>ham</td>\n",
       "      <td>Madam,regret disturbance.might receive a refer...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>spam</td>\n",
       "      <td>Oh my god! I've found your number again! I'm s...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>spam</td>\n",
       "      <td>Hi babe its Chloe, how r u? I was smashed on s...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>ham</td>\n",
       "      <td>staff.science.nus.edu.sg/~phyhcmk/teaching/pc1323</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>ham</td>\n",
       "      <td>Garbage bags, eggs, jam, bread, hannaford whea...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>ham</td>\n",
       "      <td>Networking technical support associate.</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>ham</td>\n",
       "      <td>Gibbs unsold.mike hussey</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>ham</td>\n",
       "      <td>Except theres a chick with huge boobs.</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>spam</td>\n",
       "      <td>0A$NETWORKS allow companies to bill for SMS, s...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>spam</td>\n",
       "      <td>Hello. We need some posh birds and chaps to us...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>ham</td>\n",
       "      <td>Raviyog Peripherals bhayandar east</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>ham</td>\n",
       "      <td>645</td>\n",
       "      <td>This needs human evaluation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                                SMS  \\\n",
       "114   spam  Not heard from U4 a while. Call me now am here...   \n",
       "135   spam  More people are dogging in your area now. Call...   \n",
       "152    ham                  Unlimited texts. Limited minutes.   \n",
       "159    ham                                       26th OF JULY   \n",
       "182    ham                         Surely result will offer:)   \n",
       "247    ham                          Which channel:-):-):):-).   \n",
       "284    ham                             Nokia phone is lovly..   \n",
       "293    ham  A Boy loved a gal. He propsd bt she didnt mind...   \n",
       "302    ham                   No calls..messages..missed calls   \n",
       "304    ham          This phone has the weirdest auto correct.   \n",
       "319    ham  We have sent JD for Customer Service cum Accou...   \n",
       "331    ham                          Just taste fish curry :-P   \n",
       "351    ham                     No. Yes please. Been swimming?   \n",
       "466   spam  You won't believe it but it's true. It's Incre...   \n",
       "492    ham  Madam,regret disturbance.might receive a refer...   \n",
       "504   spam  Oh my god! I've found your number again! I'm s...   \n",
       "546   spam  Hi babe its Chloe, how r u? I was smashed on s...   \n",
       "605    ham  staff.science.nus.edu.sg/~phyhcmk/teaching/pc1323   \n",
       "667    ham  Garbage bags, eggs, jam, bread, hannaford whea...   \n",
       "689    ham            Networking technical support associate.   \n",
       "706    ham                           Gibbs unsold.mike hussey   \n",
       "723    ham             Except theres a chick with huge boobs.   \n",
       "741   spam  0A$NETWORKS allow companies to bill for SMS, s...   \n",
       "953   spam  Hello. We need some posh birds and chaps to us...   \n",
       "983    ham                 Raviyog Peripherals bhayandar east   \n",
       "1063   ham                                                645   \n",
       "\n",
       "                       prediction  \n",
       "114                           ham  \n",
       "135                           ham  \n",
       "152                          spam  \n",
       "159                          spam  \n",
       "182                          spam  \n",
       "247                          spam  \n",
       "284                          spam  \n",
       "293   This needs human evaluation  \n",
       "302                          spam  \n",
       "304                          spam  \n",
       "319                          spam  \n",
       "331                          spam  \n",
       "351                          spam  \n",
       "466                           ham  \n",
       "492                          spam  \n",
       "504                           ham  \n",
       "546                           ham  \n",
       "605                          spam  \n",
       "667                          spam  \n",
       "689                          spam  \n",
       "706                          spam  \n",
       "723                          spam  \n",
       "741                           ham  \n",
       "953                           ham  \n",
       "983                          spam  \n",
       "1063  This needs human evaluation  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[test_set.Label != test_set.prediction]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's try out different alpha values to see how the accuracy changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>98.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3</td>\n",
       "      <td>98.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6</td>\n",
       "      <td>97.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>97.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.3</td>\n",
       "      <td>97.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.6</td>\n",
       "      <td>97.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>97.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alpha  Accuracy\n",
       "0    0.1      98.3\n",
       "1    0.3      98.3\n",
       "2    0.6      97.9\n",
       "3    1.0      97.6\n",
       "4    1.3      97.8\n",
       "5    1.6      97.7\n",
       "6    2.0      97.7"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_value = pd.DataFrame()\n",
    "alpha_value['alpha'] = [0.1, 0.3, 0.6, 1, 1.3, 1.6, 2]\n",
    "alpha_value['Accuracy'] = [98.3, 98.3, 97.9, 97.6, 97.8, 97.7, 97.7]\n",
    "alpha_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems our accuracy reach its peak with a smaller value of alpha. It proves that our classifier only see a minority of words it never seen before in the train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
